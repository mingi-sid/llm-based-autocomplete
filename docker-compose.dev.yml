services:
  server:
    image: llm-based-autocomplete-server
    build:
      context: ./server
      dockerfile: ./Dockerfile
    ports:
      - "5000:5000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
  client:
    image: llm-based-autocomplete-client
    build:
      context: ./client/llm-based-autocomplete-client
      dockerfile: ./Dockerfile
    ports:
      - "3000:3000"
